# Feature Reduction



> 本文是我的听课笔记，课程来源： [油管——特征选择课程](https://www.youtube.com/watch?v=KTzXVnRlnw4&t=63s) 
>
>  另外:前面的课程没有看，猜测是根据距离分类。
>
> 英语术语：
>
> - feature reduction 特征降维
> - curse of dimensionality 维数灾难
> - irrelavant feature 不相关特征
> - redundant feature 冗余特征
> - feature selection 特征选择
> - feature extraction 特征提取
> - subset 子集

![在这里插入图片描述](https://img-blog.csdnimg.cn/20191210222501312.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDE5NjUzOQ==,size_16,color_FFFFFF,t_70)

---

## 一. 维数灾难的产生

> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20191210232104533.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDE5NjUzOQ==,size_16,color_FFFFFF,t_70)

### 1.1 特征包含了目标的信息

### 1.2 特征越多结果会越好吗？

答：有更多的特征 不一定意味着 有更多的信息 或 更好的区分力 / 分类能力。看下面的例子：训练集样本数目一定，随着特征数量的增加，后面反而会使分类结果变差。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20191210221445439.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDE5NjUzOQ==,size_16,color_FFFFFF,t_70)

### 1.3 为什么出现上面的情况？

答：因为有下面两种特征：

- 不相关特征(irrelavant feature)，比如噪声noise会造成误导 -> 使结果更差。

- 冗余特征(redundant feature)，它不会对“有效信息”有新的贡献，所以造成了分类能力的下降。

### 1.4 当我们的数据集有限或者计算资源有限时

选择更多的特征效果并不好会出现过拟合，出现 维数灾难 。

---

## 二. 维数灾难的解决方法

> ![](https://img-blog.csdnimg.cn/20191210232029466.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDE5NjUzOQ==,size_16,color_FFFFFF,t_70)![在这里插入图片描述](https://img-blog.csdnimg.cn/20191210231952657.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDE5NjUzOQ==,size_16,color_FFFFFF,t_70)

### 2.1 特征选择

**特征选择** 就是选择 N个特征 的一个子集作为新的特征（新的特征数=M<N）。

- 一般可能会想到：穷举所有可能的子集并检查它们的效果。这当然是不可行的，假设原来有N个特征，那么它的子集可能有2^N种，成指数增长 -> 指数爆炸！ 为此我们要找一些合理的方法：

> ![这里是引用](https://img-blog.csdnimg.cn/20191210232428334.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDE5NjUzOQ==,size_16,color_FFFFFF,t_70)

> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20191210232711850.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDE5NjUzOQ==,size_16,color_FFFFFF,t_70)

> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20191210232743715.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDE5NjUzOQ==,size_16,color_FFFFFF,t_70)

> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20191210232817105.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDE5NjUzOQ==,size_16,color_FFFFFF,t_70)

> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20191210233016392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDE5NjUzOQ==,size_16,color_FFFFFF,t_70)

 

 （1）选择子集的方法：Optimum methods，Heuristic methods，Randomized methods。当我们的假设空间（Hypothesis space）或特征子集空间（feature subset space）有一定结构时可以用 Optimum methods ，它的执行时间是多项式（polynominal）级的时间。

 （2）评估子集的方法：非监督学习方法-过滤器方法；监督学习方法-Wrapper。

- 除去不相关的特征：

 （1）前向选择：空集开始，依次遍历每次加入一个新的“贡献最大”的特征

 （2）后向选择：



### 2.2 特征提取（下节课讲）

**特征提取** 就是将 N 维的特征投影到 M 维的子空间。

### 2.3 我们的目标

降低计算成本和简化计算模型，同时可以保持甚至提高分类器的准确率。